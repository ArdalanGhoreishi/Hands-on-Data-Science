{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b48f8a4e-faea-4589-b4b7-fbedfafaf19e",
      "metadata": {
        "id": "b48f8a4e-faea-4589-b4b7-fbedfafaf19e"
      },
      "source": [
        "# Task 2: Random Data?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527419fe-8e73-4bd4-bde5-28cdcc92a861",
      "metadata": {
        "id": "527419fe-8e73-4bd4-bde5-28cdcc92a861"
      },
      "source": [
        "## Question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae778c4-8eaa-4b28-91f9-58004a71f914",
      "metadata": {
        "id": "1ae778c4-8eaa-4b28-91f9-58004a71f914"
      },
      "source": [
        "> I ran the following code for a binary classification task w/ an SVM in both R (first sample) and Python (second example).\n",
        ">\n",
        "> Given randomly generated data (X) and response (Y), this code performs leave group out cross validation 1000 times. Each entry of Y is therefore the mean of the prediction across CV iterations.\n",
        ">\n",
        "> Computing area under the curve should give ~0.5, since X and Y are completely random. However, this is not what we see. Area under the curve is frequently significantly higher than 0.5. The number of rows of X is very small, which can obviously cause problems.\n",
        ">\n",
        "> Any idea what could be happening here? I know that I can either increase the number of rows of X or decrease the number of columns to mediate the problem, but I am looking for other issues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eecb359-6799-4523-9cdd-05187e1230b2",
      "metadata": {
        "id": "7eecb359-6799-4523-9cdd-05187e1230b2"
      },
      "source": [
        "```R\n",
        "Y=as.factor(rep(c(1,2), times=14))\n",
        "X=matrix(runif(length(Y)*100), nrow=length(Y))\n",
        "\n",
        "library(e1071)\n",
        "library(pROC)\n",
        "\n",
        "colnames(X)=1:ncol(X)\n",
        "iter=1000\n",
        "ansMat=matrix(NA,length(Y),iter)\n",
        "for(i in seq(iter)){    \n",
        "    #get train\n",
        "\n",
        "    train=sample(seq(length(Y)),0.5*length(Y))\n",
        "    if(min(table(Y[train]))==0)\n",
        "    next\n",
        "\n",
        "    #test from train\n",
        "    test=seq(length(Y))[-train]\n",
        "\n",
        "    #train model\n",
        "    XX=X[train,]\n",
        "    YY=Y[train]\n",
        "    mod=svm(XX,YY,probability=FALSE)\n",
        "    XXX=X[test,]\n",
        "    predVec=predict(mod,XXX)\n",
        "    RFans=attr(predVec,'decision.values')\n",
        "    ansMat[test,i]=as.numeric(predVec)\n",
        "}\n",
        "\n",
        "ans=rowMeans(ansMat,na.rm=TRUE)\n",
        "\n",
        "r=roc(Y,ans)$auc\n",
        "print(r)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d14e4617-608b-4749-9ec5-edf15814f5bf",
      "metadata": {
        "id": "d14e4617-608b-4749-9ec5-edf15814f5bf"
      },
      "source": [
        "Similarly, when I implement the same thing in Python I get similar results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_curve, auc"
      ],
      "metadata": {
        "id": "7Gb2eHgtI8Z3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7Gb2eHgtI8Z3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf44d8d-ca36-4d6b-bafd-78cade069751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cf44d8d-ca36-4d6b-bafd-78cade069751",
        "outputId": "f6926cfb-20df-465c-a8a2-fe3d310b3277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9081632653061225\n"
          ]
        }
      ],
      "source": [
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "n_iter = 1000\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "for i in range(n_iter):\n",
        "    # Get train/test index\n",
        "    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n",
        "    if len(np.unique(Y)) == 1:\n",
        "        continue\n",
        "    test = np.array([i for i in range(len(Y)) if i not in train])\n",
        "    # train model\n",
        "    mod = SVC(probability=False)\n",
        "    mod.fit(X=X[train, :], y=Y[train])\n",
        "    # predict and collect answer\n",
        "    ansMat[test, i] = mod.predict(X[test, :])\n",
        "ans = np.nanmean(ansMat, axis=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y, ans, pos_label=1)\n",
        "print(auc(fpr, tpr))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504cde2c-c5fd-4bc8-8aba-efb044d31198",
      "metadata": {
        "id": "504cde2c-c5fd-4bc8-8aba-efb044d31198"
      },
      "source": [
        "## Your answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3-xXyPEIXow"
      },
      "id": "V3-xXyPEIXow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I aim to transparently outline my problem-solving approach for this task. My thought process involved exploring numerous possibilities and implementing various checks, including examining how predicted values were sent to roc_curve and evaluating the average AUC. However, upon completing my problem-solving journey, I concluded that the root cause of the strange results lies in the methodology used for data splitting during cross-validation. The specifics of how and why I arrived at this conclusion are detailed in the following explanation.\n",
        "\n",
        "In my initial attempt, I aimed to address the issue of sending continuous values to the ROC curve. Rather than straightforwardly averaging them, I explored two alternatives: calculating the mode value or implementing a conditional code snippet that mapped values to 2 if the average exceeded 1.5, and to 1 otherwise. This effort, however, did not yield satisfactory results. While there were slight improvements in random outcomes, the AUC values consistently remained significantly higher than 0.5 in most experiments. So this solution could not be the correct one.\n",
        "\n",
        "Following my initial attempt, it became evident that an alternative explanation was required for the peculiar results observed. I began to scrutinize the methodology employed in calculating the final AUC, suspecting that there might be a flaw in the approach. I believe each iteration of cross-validation needed to be treated as a distinct experiment, and the AUC had to be computed independently for each. The final result would then be derived from the average AUCs. So in my second attempt, I adhered to the same cross-validation method outlined in the question. However, as previously emphasized, I treated each iteration as an independent experiment. Ultimately, I computed the AUC separately for each experiment and then calculated the average AUC. This approach yielded significantly improved results and resolved the earlier issues, making it the preferred solution for this task.\n",
        "\n",
        "In my third attempt, I adopted a k-fold cross-validation with five folds to gauge the model's performance. Personally, I favored this approach because it provides a more comprehensive examination of the data, ensuring that all data points are thoroughly tested. Historically, I had achieved better results with this method. Similar to the second attempt, I calculated the average AUCs obtained from the test data across different folds to determine the final AUC. The AUC value in this third attempt also demonstrated excellence, closely approaching 0.5.\n",
        "\n",
        "\n",
        "The attempts discussed earlier did not seem to pinpoint the core issue in this question. As suggested, manipulating the size of the input data by increasing the number of rows in X or reducing the number of columns might help mitigate the problem. Therefore, it appears that the primary challenge lies in the data-splitting process. The current method of randomly assigning samples to the training and test sets may lead to imbalanced distributions, resulting in a biased model that consistently predicts one value. The interpretation of model performance, based on the pos_label parameter, can vary widely, yielding either exceptionally good or bad results.\n",
        "\n",
        "To investigate this hypothesis, I conducted a fourth experiment, altering the data splitting approach to adopt a StratifiedKFold cross-validation with five folds. This method offers a more thorough examination of the data, ensuring that all data points are comprehensively tested. Importantly, StratifiedKFold addresses the issue of imbalanced train and test splits. As mentioned in the StratifiedKFold webpage, the folds are made by preserving the percentage of samples for each class. The AUC value obtained in this attempt demonstrated excellence, closely approaching 0.5. In conclusion, it appears that the main problem was the methodology used for data splitting.\n",
        "\n",
        "Let me explain why an imbalanced training set can lead to unusual results. We have a matrix where we record predicted values (y_pred) for our X_test in each of the 1000 iterations. Under normal circumstances, if everything is working well, the final average (ans) calculated by taking the mean from the matrix columns should be around 1.5, give or take a bit.\n",
        "\n",
        "However, due to random splitting and small sample sizes, some iterations result in an imbalanced training set. This causes our model to be biased towards predicting test samples as the majority class in our training set. For instance, if the test set is imbalanced with mostly class 1 samples, and our training set majority is class 2, the model tends to predict more class 2 for class 1 samples and vice versa. Consequently, the averages for class 1 samples become greater than 1.5, and for class 2 samples, become smaller than 1.5.\n",
        "\n",
        "When we use these averages in roc_curve, it sets n+1 thresholds, with n being the number of unique samples in our ans list. The maximum threshold is the maximum element in ans + 1, and the minimum is the minimum element in ans. The other thresholds lie in between. Setting pos_label to 1 for many thresholds results in a high True Positive Rate (TPR) and low False Positive Rate (FPR), leading to a high Area Under the Curve (AUC). Conversely, setting pos_label to 2 yields a low TPR and high FPR, resulting in a very low AUC.\n",
        "\n",
        "To address this issue, we can use different cross-validation techniques, such as stratified k-fold. Although stratified k-fold ensures that each sample is used as a test set exactly once, automatically handling the problem of averaging.\n",
        "\n",
        "In conclusion, the fundamental issue in this question appears to stem from the unconventional method of data splitting, where a random selection of training and testing sets with limited data rows can lead to imbalances, consequently biasing the model's predictions. Moreover, the strategy of averaging 'y_pred' values and applying this average in ROC analysis is questioned, as treating each cross-validation iteration as a distinct experiment and independently calculating AUC for each seems to offer a more robust and reliable evaluation. The provided code for all attempts is detailed below."
      ],
      "metadata": {
        "id": "y-K-ur5kg6XM"
      },
      "id": "y-K-ur5kg6XM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First attempt"
      ],
      "metadata": {
        "id": "8ZIWZrssW1U4"
      },
      "id": "8ZIWZrssW1U4"
    },
    {
      "cell_type": "code",
      "source": [
        "def nanmode(arr):\n",
        "    mask = ~np.isnan(arr)  # Create a mask to exclude NaN values\n",
        "    if np.any(mask):\n",
        "        return stats.mode(arr[mask]).mode[0]  # Calculate mode for non-NaN values\n",
        "    else:\n",
        "        return np.nan  # If all values are NaN, return NaN"
      ],
      "metadata": {
        "id": "QKRwDo2FyiO9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QKRwDo2FyiO9"
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 1.5  # Choose an appropriate threshold value"
      ],
      "metadata": {
        "id": "Bkm6jnFNoCih"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Bkm6jnFNoCih"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "492960dd-ecf1-4110-a016-cf550ed1d191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FybHbfo0IYBg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.7142857142857143\n"
          ]
        }
      ],
      "source": [
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "n_iter = 1000\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "for i in range(n_iter):\n",
        "    # Get train/test index\n",
        "    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n",
        "    if len(np.unique(Y)) == 1:\n",
        "        continue\n",
        "    test = np.array([i for i in range(len(Y)) if i not in train])\n",
        "    # train model\n",
        "    mod = SVC(probability=False)\n",
        "    mod.fit(X=X[train, :], y=Y[train])\n",
        "    # predict and collect answer\n",
        "    ansMat[test, i] = mod.predict(X[test, :])\n",
        "ans = np.nanmean(ansMat, axis=1)\n",
        "binary_predictions = np.where(ans >= threshold, 2, 1)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(Y, binary_predictions, pos_label=1)\n",
        "# ans = np.apply_along_axis(nanmode, axis=1, arr=ansMat)\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y, ans, pos_label=1)\n",
        "print(\"AUC: \",metrics.auc(fpr, tpr))"
      ],
      "id": "FybHbfo0IYBg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second attempt"
      ],
      "metadata": {
        "id": "piEU63bpWxzg"
      },
      "id": "piEU63bpWxzg"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "\n",
        "n_iter = 1000  # Number of cross-validation iterations\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "\n",
        "\n",
        "# Perform task's custom cross-validation\n",
        "for i in range(n_iter):\n",
        "    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n",
        "    if len(np.unique(Y)) == 1:\n",
        "        continue\n",
        "    test = np.array([i for i in range(len(Y)) if i not in train])\n",
        "    # Train/test split logic here...\n",
        "\n",
        "    clf = SVC(probability=False)\n",
        "    clf.fit(X=X[train, :], y=Y[train])\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = clf.predict(X[test, :])\n",
        "\n",
        "    # Compute ROC curve and AUC for this iteration\n",
        "    fpr, tpr, _ = metrics.roc_curve(Y[test], y_pred, pos_label=2)\n",
        "    fold_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # Append AUC to the result matrix\n",
        "    ansMat[test, i] = fold_auc\n",
        "\n",
        "# Calculate the mean AUC across all iterations\n",
        "mean_auc = np.nanmean(ansMat)\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5QmxSyOuX55",
        "outputId": "90a709cd-79da-41ae-e9c1-14ca04df3140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean AUC: 0.543625\n"
          ]
        }
      ],
      "id": "F5QmxSyOuX55"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third attempt"
      ],
      "metadata": {
        "id": "BeV5Bln-W_0v"
      },
      "id": "BeV5Bln-W_0v"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "\n",
        "n_splits = 5  # Choose an appropriate number of splits\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize list to store AUC for each fold\n",
        "auc_values = []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_idx, test_idx in cv.split(X, Y):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
        "\n",
        "    clf = SVC(probability=False)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Compute ROC curve and AUC for this fold\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=2)\n",
        "    fold_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Append AUC to the list\n",
        "    auc_values.append(fold_auc)\n",
        "\n",
        "# Calculate the mean AUC across all folds\n",
        "mean_auc = np.mean(auc_values)\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0dsg4eeruGw",
        "outputId": "d5d79ab3-5f02-4131-b7ef-87cc31a3c609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean AUC: 0.5\n"
          ]
        }
      ],
      "id": "B0dsg4eeruGw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth attempt"
      ],
      "metadata": {
        "id": "j3VBkYhHIdl5"
      },
      "id": "j3VBkYhHIdl5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "082f817e-55be-494c-a316-93c4042ed41a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "082f817e-55be-494c-a316-93c4042ed41a",
        "outputId": "d1f4b1f8-4a00-4629-dcec-3065433e0aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "\n",
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "n_iter = 1000\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "\n",
        "# Use StratifiedKFold for better data splitting\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for i, (train, test) in enumerate(skf.split(X, Y)):\n",
        "    # train model\n",
        "    mod = SVC(probability=False)\n",
        "    mod.fit(X[train, :], Y[train])\n",
        "    # predict and collect answer\n",
        "    ansMat[test, i] = mod.predict(X[test, :])\n",
        "\n",
        "ans = np.nanmean(ansMat, axis=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y, ans, pos_label=2)\n",
        "print(\"AUC: \",auc(fpr, tpr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfb3ad1-3f67-403e-9ee4-775b67f76660",
      "metadata": {
        "id": "fcfb3ad1-3f67-403e-9ee4-775b67f76660"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "867962a5-b639-4d39-882c-5a42a68e891c",
      "metadata": {
        "id": "867962a5-b639-4d39-882c-5a42a68e891c"
      },
      "source": [
        "Was this exercise is difficult or not? In either case, briefly describe why."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I prefer not to categorize this task as either difficult or easy. Instead, I find it more fitting to describe it as an enjoyable and challenging endeavor that requires a unique approach. While I cannot guarantee that my response aligns precisely with your expectations, I appreciate the opportunity to engage in problem-solving that encourages a shift in perspective. It felt like a mental workout, and I relish the chance to navigate such stimulating paths."
      ],
      "metadata": {
        "id": "YPdK4rA6MDMT"
      },
      "id": "YPdK4rA6MDMT"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}