{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b48f8a4e-faea-4589-b4b7-fbedfafaf19e",
      "metadata": {
        "id": "b48f8a4e-faea-4589-b4b7-fbedfafaf19e"
      },
      "source": [
        "# Task 2: Why are the predictions to good (/bad)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527419fe-8e73-4bd4-bde5-28cdcc92a861",
      "metadata": {
        "id": "527419fe-8e73-4bd4-bde5-28cdcc92a861"
      },
      "source": [
        "## Question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae778c4-8eaa-4b28-91f9-58004a71f914",
      "metadata": {
        "id": "1ae778c4-8eaa-4b28-91f9-58004a71f914"
      },
      "source": [
        "> I ran the following code for a binary classification task w/ an SVM in both R (first sample) and Python (second example).\n",
        ">\n",
        "> Given randomly generated data (X) and response (Y), this code performs leave group out cross validation 1000 times. Each entry of Y is therefore the mean of the prediction across CV iterations.\n",
        ">\n",
        "> Computing area under the curve should give ~0.5, since X and Y are completely random. However, this is not what we see. Area under the curve is frequently significantly higher than 0.5. The number of rows of X is very small, which can obviously cause problems.\n",
        ">\n",
        "> Any idea what could be happening here? I know that I can either increase the number of rows of X or decrease the number of columns to mediate the problem, but I am looking for other issues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eecb359-6799-4523-9cdd-05187e1230b2",
      "metadata": {
        "id": "7eecb359-6799-4523-9cdd-05187e1230b2"
      },
      "source": [
        "```R\n",
        "Y=as.factor(rep(c(1,2), times=14))\n",
        "X=matrix(runif(length(Y)*100), nrow=length(Y))\n",
        "\n",
        "library(e1071)\n",
        "library(pROC)\n",
        "\n",
        "colnames(X)=1:ncol(X)\n",
        "iter=1000\n",
        "ansMat=matrix(NA,length(Y),iter)\n",
        "for(i in seq(iter)){    \n",
        "    #get train\n",
        "\n",
        "    train=sample(seq(length(Y)),0.5*length(Y))\n",
        "    if(min(table(Y[train]))==0)\n",
        "    next\n",
        "\n",
        "    #test from train\n",
        "    test=seq(length(Y))[-train]\n",
        "\n",
        "    #train model\n",
        "    XX=X[train,]\n",
        "    YY=Y[train]\n",
        "    mod=svm(XX,YY,probability=FALSE)\n",
        "    XXX=X[test,]\n",
        "    predVec=predict(mod,XXX)\n",
        "    RFans=attr(predVec,'decision.values')\n",
        "    ansMat[test,i]=as.numeric(predVec)\n",
        "}\n",
        "\n",
        "ans=rowMeans(ansMat,na.rm=TRUE)\n",
        "\n",
        "r=roc(Y,ans)$auc\n",
        "print(r)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d14e4617-608b-4749-9ec5-edf15814f5bf",
      "metadata": {
        "id": "d14e4617-608b-4749-9ec5-edf15814f5bf"
      },
      "source": [
        "Similarly, when I implement the same thing in Python I get similar results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "paimFmS0igoW"
      },
      "id": "paimFmS0igoW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "287bb69e-cfdf-48d8-9a4d-37189699bce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CrQGkaBV6vw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9591836734693877\n"
          ]
        }
      ],
      "source": [
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "n_iter = 1000\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "for i in range(n_iter):\n",
        "    # Get train/test index\n",
        "    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n",
        "    if len(np.unique(Y)) == 1:\n",
        "        continue\n",
        "    test = np.array([i for i in range(len(Y)) if i not in train])\n",
        "    # train model\n",
        "    mod = SVC(probability=False)\n",
        "    mod.fit(X=X[train, :], y=Y[train])\n",
        "    # predict and collect answer\n",
        "    ansMat[test, i] = mod.predict(X[test, :])\n",
        "ans = np.nanmean(ansMat, axis=1)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(Y, ans, pos_label=1)\n",
        "print(metrics.auc(fpr, tpr))"
      ],
      "id": "-CrQGkaBV6vw"
    },
    {
      "cell_type": "markdown",
      "id": "504cde2c-c5fd-4bc8-8aba-efb044d31198",
      "metadata": {
        "id": "504cde2c-c5fd-4bc8-8aba-efb044d31198"
      },
      "source": [
        "## Your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My initial focus when tackling this task was centered on determining an appropriate method for calculating 'y_pred' based on values obtained during cross-validation. The utilized approach of simply averaging these values in a binary classification task did not seem to align with best practices. Sending continuous values as predictions for the ROC curve could introduce inherent challenges, though it's important to note that this was just one of several concerns. Each iteration of cross-validation needed to be treated as a distinct experiment, and the AUC had to be computed independently for each. The final result would then be derived from the average AUCs. To rigorously assess this task, I conducted practical testing for all the proposed solutions.\n",
        "\n",
        "In my initial attempt, I aimed to address the issue of sending continuous values to the ROC curve. Rather than straightforwardly averaging them, I explored two alternatives: calculating the mode value or implementing a conditional code snippet that mapped values to 2 if the average exceeded 1.5, and to 1 otherwise. This effort, however, did not yield satisfactory results. While there were slight improvements in random outcomes, the AUC values consistently remained significantly higher than 0.5 in most experiments. So this solution could not be the correct one.\n",
        "\n",
        "In my second attempt, I adhered to the same cross-validation method outlined in the question. However, as previously emphasized, I treated each iteration as an independent experiment. Ultimately, I computed the AUC separately for each experiment and then calculated the average AUC. This approach yielded significantly improved results and resolved the earlier issues, making it the preferred solution for this task.\n",
        "\n",
        "In my third attempt, I adopted a k-fold cross-validation with five folds to gauge the model's performance. Personally, I favored this approach because it provides a more comprehensive examination of the data, ensuring that all data points are thoroughly tested. Historically, I had achieved better results with this method. Similar to the second attempt, I calculated the average AUCs obtained from the test data across different folds to determine the final AUC. The AUC value in this third attempt also demonstrated excellence, closely approaching 0.5.\n",
        "\n",
        "\n",
        "In summary, the peculiar behavior observed in the mentioned code can be attributed primarily to the unconventional use of cross-validation. The random selection of training and testing sets with limited data rows can lead to issues, especially if the training data is imbalanced. Additionally, I hold the conviction that the employed strategy of averaging 'y_pred' values and subsequently applying this average in ROC analysis is not sound. It is preferable to treat each iteration in the cross-validation process as a distinct experiment, independently calculating the AUC for each. The final result should be obtained by averaging these individual AUCs, ensuring a more accurate and reliable outcome. The code for all the aforementioned attempts is provided below:"
      ],
      "metadata": {
        "id": "y-K-ur5kg6XM"
      },
      "id": "y-K-ur5kg6XM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First attempt"
      ],
      "metadata": {
        "id": "8ZIWZrssW1U4"
      },
      "id": "8ZIWZrssW1U4"
    },
    {
      "cell_type": "code",
      "source": [
        "def nanmode(arr):\n",
        "    mask = ~np.isnan(arr)  # Create a mask to exclude NaN values\n",
        "    if np.any(mask):\n",
        "        return stats.mode(arr[mask]).mode[0]  # Calculate mode for non-NaN values\n",
        "    else:\n",
        "        return np.nan  # If all values are NaN, return NaN"
      ],
      "metadata": {
        "id": "QKRwDo2FyiO9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QKRwDo2FyiO9"
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 1.5  # Choose an appropriate threshold value"
      ],
      "metadata": {
        "id": "Bkm6jnFNoCih"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Bkm6jnFNoCih"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cf44d8d-ca36-4d6b-bafd-78cade069751",
        "outputId": "9d97ddbc-33a9-4270-8918-f9cd66fb40dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6071428571428571\n"
          ]
        }
      ],
      "source": [
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "n_iter = 1000\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "for i in range(n_iter):\n",
        "    # Get train/test index\n",
        "    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n",
        "    if len(np.unique(Y)) == 1:\n",
        "        continue\n",
        "    test = np.array([i for i in range(len(Y)) if i not in train])\n",
        "    # train model\n",
        "    mod = SVC(probability=False)\n",
        "    mod.fit(X=X[train, :], y=Y[train])\n",
        "    # predict and collect answer\n",
        "    ansMat[test, i] = mod.predict(X[test, :])\n",
        "ans = np.nanmean(ansMat, axis=1)\n",
        "binary_predictions = np.where(ans >= threshold, 2, 1)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(Y, binary_predictions, pos_label=1)\n",
        "# ans = np.apply_along_axis(nanmode, axis=1, arr=ansMat)\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y, ans, pos_label=1)\n",
        "print(metrics.auc(fpr, tpr))"
      ],
      "id": "0cf44d8d-ca36-4d6b-bafd-78cade069751"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second attempt"
      ],
      "metadata": {
        "id": "piEU63bpWxzg"
      },
      "id": "piEU63bpWxzg"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "\n",
        "n_iter = 1000  # Number of cross-validation iterations\n",
        "ansMat = np.full((len(Y), n_iter), np.nan)\n",
        "\n",
        "\n",
        "# Perform your custom cross-validation\n",
        "for i in range(n_iter):\n",
        "    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n",
        "    if len(np.unique(Y)) == 1:\n",
        "        continue\n",
        "    test = np.array([i for i in range(len(Y)) if i not in train])\n",
        "    # Train/test split logic here...\n",
        "\n",
        "    # Train your classifier (e.g., SVM)\n",
        "    clf = SVC(probability=False)\n",
        "    clf.fit(X=X[train, :], y=Y[train])\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = clf.predict(X[test, :])\n",
        "\n",
        "    # Compute ROC curve and AUC for this iteration\n",
        "    fpr, tpr, _ = metrics.roc_curve(Y[test], y_pred, pos_label=2)  # Adjust pos_label accordingly\n",
        "    fold_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # Append AUC to the result matrix\n",
        "    ansMat[test, i] = fold_auc\n",
        "\n",
        "# Calculate the mean AUC across all iterations\n",
        "mean_auc = np.nanmean(ansMat)\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5QmxSyOuX55",
        "outputId": "e50190dd-086f-48c2-bfc3-737e84dd7d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean AUC: 0.5344166666666667\n"
          ]
        }
      ],
      "id": "F5QmxSyOuX55"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third attempt"
      ],
      "metadata": {
        "id": "BeV5Bln-W_0v"
      },
      "id": "BeV5Bln-W_0v"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "Y = np.array([1, 2]*14)\n",
        "X = np.random.uniform(size=[len(Y), 100])\n",
        "\n",
        "n_splits = 5  # Choose an appropriate number of splits\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize list to store AUC for each fold\n",
        "auc_values = []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_idx, test_idx in cv.split(X, Y):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
        "\n",
        "    # Train your classifier (e.g., SVM)\n",
        "    clf = SVC(probability=False)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Compute ROC curve and AUC for this fold\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=2)  # Adjust pos_label accordingly\n",
        "    fold_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Append AUC to the list\n",
        "    auc_values.append(fold_auc)\n",
        "\n",
        "# Calculate the mean AUC across all folds\n",
        "mean_auc = np.mean(auc_values)\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0dsg4eeruGw",
        "outputId": "af964bf4-c978-4f1e-be16-e86cc6cb69f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean AUC: 0.5\n"
          ]
        }
      ],
      "id": "B0dsg4eeruGw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a25275-359a-45bd-99f5-647ea3650b95",
      "metadata": {
        "id": "c8a25275-359a-45bd-99f5-647ea3650b95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fcfb3ad1-3f67-403e-9ee4-775b67f76660",
      "metadata": {
        "id": "fcfb3ad1-3f67-403e-9ee4-775b67f76660"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "867962a5-b639-4d39-882c-5a42a68e891c",
      "metadata": {
        "id": "867962a5-b639-4d39-882c-5a42a68e891c"
      },
      "source": [
        "Was this exercise is difficult or not? In either case, briefly describe why."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task initially appeared quite challenging, but upon closer examination, it revealed itself to be a thought-provoking and conceptual question that required a deeper level of contemplation to answer effectively."
      ],
      "metadata": {
        "id": "anCyEdpFWLIm"
      },
      "id": "anCyEdpFWLIm"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nalab-milk",
      "language": "python",
      "name": "nalab-milk"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}